{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/eftekhar-hossain/Disaster_IEEE-Access/blob/main/damage_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D09VDHN9xiww"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OdpHrd7yReb",
    "outputId": "7eabf2bc-44a2-40d7-9166-996283a53adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 ms, sys: 807 µs, total: 2.45 ms\n",
      "Wall time: 1.8 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zaima/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re,nltk,json\n",
    "from sklearn.metrics import confusion_matrix,classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
    "from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\n",
    "from keras.utils.vis_utils import plot_model\n",
    "np.random.seed(42)\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import string, spacy,unicodedata, random\n",
    "from bs4 import BeautifulSoup\n",
    "class color: # Text style\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "### Pretrained Word Embeddings\n",
    "pretrained_path = \"/Users/zaima/Desktop/Dataset/PreTrained Wordembedding/\"\n",
    "# dataset_path = \"/content/drive/MyDrive/Colab Notebooks/MSC Presentation Tasks/Disaster Response/Images/\"\n",
    "# folder_path = \"/content/drive/MyDrive/Colab Notebooks/MSC Presentation Tasks/Disaster Response/\"\n",
    "models_path = \"/Users/zaima/Desktop/Dataset/Multimodal Sentiment/\"\n",
    "\n",
    "\n",
    "folder_path = \"/Users/zaima/Desktop/Dataset/\"\n",
    "# models_path = \"/content/drive/MyDrive/TaheriThesis/Dataset/Multimodal Sentiment/Models/\"\n",
    "# results_path = \"/content/drive/MyDrive/TaheriThesis/Dataset/Multimodal Sentiment/Results/\"\n",
    "dataset_path = \"/Users/zaima/Desktop/Dataset/Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "l767bYFbGNLf"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Embedding, LSTM, multiply\n",
    "from keras.models import Model\n",
    "from keras import preprocessing, Input\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "#Import from keras_preprocessing not from keras.preprocessing\n",
    "# from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "import itertools\n",
    "from PIL import Image, ImageFile\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Convolution1D,MaxPooling1D,Conv1D\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Add, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Nadam\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjImoyXizNSk"
   },
   "source": [
    "#Fetching Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nt9hWjXNzQxq"
   },
   "outputs": [],
   "source": [
    "# train_data = pd.read_excel(folder_path+'train.xlsx')\n",
    "# test_data = pd.read_csv(folder_path+'test.xlsx')\n",
    "# valid_data = pd.read_excel(main_path+'valid_new.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(folder_path+'train.csv')\n",
    "valid_data = pd.read_csv(folder_path+'val.csv')\n",
    "test_data = pd.read_csv(folder_path+'test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lDAsGYgMs8r",
    "outputId": "2485a63b-0db8-4652-c8c0-bdac9c50fdca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_name', 'Captions', 'Label_Sentiment', 'Label'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhUgGvUa2WFX",
    "outputId": "46c3fe03-08a2-43fd-f3fb-3e0d5feaf889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    861\n",
       "6    716\n",
       "4    623\n",
       "1    477\n",
       "5    408\n",
       "3    400\n",
       "2    348\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPO59gZr2c5p",
    "outputId": "f0b4afd9-a7e7-4410-ffad-eae4b57b3838"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    93\n",
       "6    79\n",
       "4    68\n",
       "5    46\n",
       "3    44\n",
       "1    44\n",
       "2    40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qJO4yFDY2nTP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_data['enc_label'] = train_data['Label_Sentiment'].replace({'happy':0,'angry':1,'disgust':2,'fear':3,'sad':4,'surprise':5,'other':6})\n",
    "valid_data['enc_label'] = valid_data['Label_Sentiment'].replace({'happy':0,'angry':1,'disgust':2,'fear':3,'sad':4,'surprise':5,'other':6})\n",
    "test_data['enc_label'] = test_data['Label_Sentiment'].replace({'happy':0,'angry':1,'disgust':2,'fear':3,'sad':4,'surprise':5,'other':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqXJL1QY3Gte",
    "outputId": "e9c0aaae-7e7e-4f16-dbb1-5259677f457f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data ==>  (3833, 5)\n",
      "Size of the Test data ==> (414, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the training data ==> \", train_data.shape)\n",
    "print(\"Size of the Test data ==>\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f7tj6t_MbR2"
   },
   "source": [
    "## Image Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ER4zNOmfMe_8"
   },
   "outputs": [],
   "source": [
    "## collect image names from the folders\n",
    "def create_img_array(img_dirct):\n",
    "    all_imgs = []\n",
    "    for root, j, files in os.walk(img_dirct):\n",
    "        for file in files:\n",
    "            file = root + '' + file\n",
    "            all_imgs.append(file)\n",
    "    return all_imgs\n",
    "\n",
    "def create_img_path(DF, Col_name, img_dir):\n",
    "    img_path = [img_dir + '' + name for name in DF[Col_name]]\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1X--YjxFMo4v"
   },
   "outputs": [],
   "source": [
    "# Creating train, test and validation image path\n",
    "train_img_path = create_img_path(train_data,'image_name', dataset_path)\n",
    "valid_img_path = create_img_path(valid_data,'image_name', dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4RLito_j4WxC"
   },
   "outputs": [],
   "source": [
    "test_img_path = create_img_path(test_data,'image_name', dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "t3FUNU_bM7Tn"
   },
   "outputs": [],
   "source": [
    "# Function that returns image reading from the path\n",
    "def get_input(path):\n",
    "    # Loading image from given path\n",
    "    # and resizing it to 150*150*3 format\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    img= tf.keras.utils.load_img(path, target_size=(224,224)) \n",
    "    # img.close()   \n",
    "    return(img)\n",
    "\n",
    "# Takes in image and preprocess it\n",
    "def process_input(img):\n",
    "    # Converting image to array    \n",
    "    img_data =tf.keras.utils.img_to_array(img)\n",
    "    # Adding one more dimension to array    \n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    #     \n",
    "    img_data = preprocess_input(img_data)\n",
    "    #img_data = preprocess_input(img_data)\n",
    "    return(img_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "pUdvKi_yNCb5"
   },
   "outputs": [],
   "source": [
    "# Create an array of training images\n",
    "train_images = []\n",
    "for n,i in enumerate(train_img_path):\n",
    "  input_img = get_input(i)\n",
    "  process_img = process_input(input_img)\n",
    "  # print(n)\n",
    "  train_images.append(process_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZdaVOmrw1CpK"
   },
   "outputs": [],
   "source": [
    "# Create an array of training images\n",
    "valid_images = []\n",
    "for n,i in enumerate(valid_img_path):\n",
    "  input_img = get_input(i)\n",
    "  process_img = process_input(input_img)\n",
    "  # print(n)\n",
    "  valid_images.append(process_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "KeGcMyaKY7s_"
   },
   "outputs": [],
   "source": [
    "# Create an array of training images\n",
    "test_images = []\n",
    "for n,i in enumerate(test_img_path):\n",
    "  input_img = get_input(i)\n",
    "  process_img = process_input(input_img)\n",
    "  # print(n)\n",
    "  test_images.append(process_img[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fd31qUec4o6j"
   },
   "outputs": [],
   "source": [
    "# convert into numpy array\n",
    "train_image = np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "EkmubC2u1Vhn"
   },
   "outputs": [],
   "source": [
    "valid_image = np.array(valid_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "G731BdDt4p5E"
   },
   "outputs": [],
   "source": [
    "test_image = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PTGSeCyIb3i0"
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(dataset_path+'train.pkl','wb') as f:\n",
    "    pkl.dump(train_image, f)\n",
    "\n",
    "# with open(dataset_path+'test.pkl','wb') as f:\n",
    "#     pkl.dump(test_image, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "en4g5tsp1ahU"
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(dataset_path+'valid.pkl','wb') as f:\n",
    "    pkl.dump(valid_image, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "cT1sl0lZx_KG"
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "with open(dataset_path+'test.pkl','wb') as f:\n",
    "    pkl.dump(test_image, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKDZjc1uxvvh",
    "outputId": "dada7b13-77cd-4248-c129-f687dc475502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images:--  (3833, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(dataset_path+'train.pkl','rb') as f:\n",
    "  train_image = pkl.load(f)\n",
    "  print(\"Training Images:-- \",train_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cbh76eTW1v4M",
    "outputId": "e7b62b0d-1813-435e-ac7f-147f4398ed44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Images:--  (414, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(dataset_path+'valid.pkl','rb') as f:\n",
    "  valid_image = pkl.load(f)\n",
    "  print(\"Validation Images:-- \",valid_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwok956k44jA",
    "outputId": "921d4f59-7507-4a91-af22-f4bb91429b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Images:--  (414, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_path+'test.pkl','rb') as f:\n",
    "  test_image = pkl.load(f)\n",
    "  print(\"Test Images:-- \",test_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C55eRoK23rzC"
   },
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "dNGZabRk3vSN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vfd2Z0c-3x6_"
   },
   "outputs": [],
   "source": [
    "# encoder=OneHotEncoder(sparse=False)\n",
    "\n",
    "# encoded_labels = pd.DataFrame (encoder.fit_transform(train_data[['label']]))\n",
    "\n",
    "# encoded_labels .columns = encoder.get_feature_names(['label'])\n",
    "\n",
    "# train_data= pd.concat([train_data, encoded_labels ], axis=1)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the list of categories for your Label_Sentiment variable\n",
    "categories = ['happy', 'angry', 'disgust', 'fear', 'sad', 'surprise', 'other']\n",
    "\n",
    "# Perform one-hot encoding using get_dummies() function\n",
    "encoded_labels = pd.get_dummies(train_data['Label_Sentiment'], columns=categories, prefix='Label_Sentiment')\n",
    "\n",
    "# Concatenate the encoded labels with your original train_data DataFrame\n",
    "train_data = pd.concat([train_data, encoded_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmdmEo9033SX",
    "outputId": "bcfa9d0e-f34f-4e7d-fabf-7099ec2a8d67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_name', 'Captions', 'Label_Sentiment', 'Label', 'enc_label',\n",
       "       'Label_Sentiment_angry', 'Label_Sentiment_disgust',\n",
       "       'Label_Sentiment_fear', 'Label_Sentiment_happy',\n",
       "       'Label_Sentiment_other', 'Label_Sentiment_sad',\n",
       "       'Label_Sentiment_surprise'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "alEhfx5H3-tX",
    "outputId": "a241d0dd-db06-40da-dda6-f0f2dbb0a4e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>Captions</th>\n",
       "      <th>Label_Sentiment</th>\n",
       "      <th>Label</th>\n",
       "      <th>enc_label</th>\n",
       "      <th>angry</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>happy</th>\n",
       "      <th>other</th>\n",
       "      <th>sad</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205.jpg</td>\n",
       "      <td>নির্বোধ দেখার এবং আপনি যে যত্নশীল তা না করার ক...</td>\n",
       "      <td>other</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209.jpg</td>\n",
       "      <td>একটি বোর্ডার আর্ম স্কেটার একটি আমন্ত্রণ উপার্জ...</td>\n",
       "      <td>other</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>210.jpg</td>\n",
       "      <td>যদি শরীর দুর্বল হয় তবে মন শক্তিশালী হবে না।</td>\n",
       "      <td>other</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215.jpg</td>\n",
       "      <td>একজন ধার্মিক ব্যক্তির কার্যকর উত্সাহ প্রার্থনা...</td>\n",
       "      <td>other</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222.jpg</td>\n",
       "      <td>অসমাপ্ত ?</td>\n",
       "      <td>other</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828</th>\n",
       "      <td>4676.jpg</td>\n",
       "      <td>অভিনন্দন ২ প্রথম আইবিএম প্যাথওয়েসটেক পণ্ডিতরা...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>4677.jpg</td>\n",
       "      <td>সবেমাত্র আমার চুলের সাথে রুটোশায়ার স্ক্যাল্প ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>4678.jpg</td>\n",
       "      <td>সবেমাত্র এই ওক ড্রেসারটি শেষ করেছেন।</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>4682.jpg</td>\n",
       "      <td>সুগারম্যান রজার্স মামলা-মোকদ্দমা কেমব্রিজে গত ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>4704.jpg</td>\n",
       "      <td>সন্ত্রাসীদের আতঙ্কিত করা উচিত। কী সেই নাজি প্র...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3833 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name                                           Captions  \\\n",
       "0       205.jpg  নির্বোধ দেখার এবং আপনি যে যত্নশীল তা না করার ক...   \n",
       "1       209.jpg  একটি বোর্ডার আর্ম স্কেটার একটি আমন্ত্রণ উপার্জ...   \n",
       "2       210.jpg       যদি শরীর দুর্বল হয় তবে মন শক্তিশালী হবে না।   \n",
       "3       215.jpg  একজন ধার্মিক ব্যক্তির কার্যকর উত্সাহ প্রার্থনা...   \n",
       "4       222.jpg                                          অসমাপ্ত ?   \n",
       "...         ...                                                ...   \n",
       "3828   4676.jpg  অভিনন্দন ২ প্রথম আইবিএম প্যাথওয়েসটেক পণ্ডিতরা...   \n",
       "3829   4677.jpg  সবেমাত্র আমার চুলের সাথে রুটোশায়ার স্ক্যাল্প ...   \n",
       "3830   4678.jpg               সবেমাত্র এই ওক ড্রেসারটি শেষ করেছেন।   \n",
       "3831   4682.jpg  সুগারম্যান রজার্স মামলা-মোকদ্দমা কেমব্রিজে গত ...   \n",
       "3832   4704.jpg  সন্ত্রাসীদের আতঙ্কিত করা উচিত। কী সেই নাজি প্র...   \n",
       "\n",
       "     Label_Sentiment  Label  enc_label  angry  disgust   fear  happy  other  \\\n",
       "0              other      6          6  False    False  False  False   True   \n",
       "1              other      6          6  False    False  False  False   True   \n",
       "2              other      6          6  False    False  False  False   True   \n",
       "3              other      6          6  False    False  False  False   True   \n",
       "4              other      6          6  False    False  False  False   True   \n",
       "...              ...    ...        ...    ...      ...    ...    ...    ...   \n",
       "3828           happy      0          0  False    False  False   True  False   \n",
       "3829           happy      0          0  False    False  False   True  False   \n",
       "3830           happy      0          0  False    False  False   True  False   \n",
       "3831           happy      0          0  False    False  False   True  False   \n",
       "3832           happy      0          0  False    False  False   True  False   \n",
       "\n",
       "        sad  surprise  \n",
       "0     False     False  \n",
       "1     False     False  \n",
       "2     False     False  \n",
       "3     False     False  \n",
       "4     False     False  \n",
       "...     ...       ...  \n",
       "3828  False     False  \n",
       "3829  False     False  \n",
       "3830  False     False  \n",
       "3831  False     False  \n",
       "3832  False     False  \n",
       "\n",
       "[3833 rows x 12 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data = train_data.rename(columns={'label_damaged_infrastructure':'damaged_infrastructure', 'label_damaged_nature':'damaged_nature', \n",
    "#                                         'label_fires':'fires','label_flood':'flood',\n",
    "#                                         'label_human_damage':'human_damage', 'label_non_damage':'non_damage'})\n",
    "\n",
    "\n",
    "train_data = train_data.rename(columns={'Label_Sentiment_happy':'happy', \n",
    "                                        'Label_Sentiment_angry':'angry',\n",
    "                                        'Label_Sentiment_disgust':'disgust',\n",
    "                                        'Label_Sentiment_fear':'fear',\n",
    "                                        'Label_Sentiment_sad':'sad',\n",
    "                                        'Label_Sentiment_surprise':'surprise',\n",
    "                                        'Label_Sentiment_other':'other'})\n",
    "     \n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-XCXOsb-zxc",
    "outputId": "106d6824-d689-4484-cc8e-af0490bf92a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_name', 'Captions', 'Label_Sentiment', 'Label', 'enc_label',\n",
       "       'angry', 'disgust', 'fear', 'happy', 'other', 'sad', 'surprise'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "48JRGZ4U6Kvv"
   },
   "outputs": [],
   "source": [
    "train_data.Captions = train_data.Captions.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22E5_M_h4fo9"
   },
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPXMlxVxGhBZ",
    "outputId": "27a7fdcf-00e4-4636-d2d5-02e02bcd6424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import locale\n",
    "print(locale.getpreferredencoding())\n",
    "\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlKxqd159uMF",
    "outputId": "20fba99e-7947-425b-bc12-fa552874bb99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in ./miniconda3/lib/python3.10/site-packages (2.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "NN2wdrH74nD7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Text Cleaning\n",
    "'''\n",
    "# import emoji\n",
    "# import re\n",
    "\n",
    "# def remove_emojis(text):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def text_cleaning(row):\n",
    "   #to remove HTML tags\n",
    "  text = BeautifulSoup(row, 'html.parser').get_text()\n",
    "  d = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE) #This line is for removing url\n",
    "  post = d.replace('\\n', '')\n",
    "  post = d.replace('\\r\\n', '')\n",
    "  post = post.replace('—', ' ') \n",
    "  post = post.replace('।', ' ')\n",
    "  text = ''.join([c for c in post if c not in string.punctuation])\n",
    "#   text= remove_emojis(text)\n",
    "  # to remove special characters\n",
    "  pattern = r'^\\s*|\\s\\s*'\n",
    "  text = re.sub(pattern, ' ', text).strip()\n",
    "  # convert into lower case\n",
    "  text = text.lower() \n",
    "  # # Stopword \n",
    "  # result = text.split()\n",
    "  # text = [word.strip() for word in result if word not in stp ]\n",
    "  # text =\" \".join(text)\n",
    "  \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_xYDRDq42TU",
    "outputId": "dba4d1cb-2ac4-4756-e413-3a7b09cc9d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples after cleaning:\n",
      "\n",
      "Original Data:===\n",
      " নির্বোধ দেখার এবং আপনি যে যত্নশীল তা না করার ক্ষমতা আছে \n",
      "Cleaned Data:===\n",
      " নির্বোধ দেখার এবং আপনি যে যত্নশীল তা না করার ক্ষমতা আছে\n",
      "Original Data:===\n",
      " একটি বোর্ডার আর্ম স্কেটার একটি আমন্ত্রণ উপার্জন করবে? \n",
      "Cleaned Data:===\n",
      " একটি বোর্ডার আর্ম স্কেটার একটি আমন্ত্রণ উপার্জন করবে\n",
      "Original Data:===\n",
      " যদি শরীর দুর্বল হয় তবে মন শক্তিশালী হবে না। \n",
      "Cleaned Data:===\n",
      " যদি শরীর দুর্বল হয় তবে মন শক্তিশালী হবে না\n",
      "Original Data:===\n",
      " একজন ধার্মিক ব্যক্তির কার্যকর উত্সাহ প্রার্থনা অনেকটা উপকার পায়। \n",
      "Cleaned Data:===\n",
      " একজন ধার্মিক ব্যক্তির কার্যকর উত্সাহ প্রার্থনা অনেকটা উপকার পায়\n",
      "Original Data:===\n",
      " অসমাপ্ত ? \n",
      "Cleaned Data:===\n",
      " অসমাপ্ত\n",
      "Original Data:===\n",
      " আপনাকে পুরো হতে নিখুঁত হতে হবে না! \n",
      "Cleaned Data:===\n",
      " আপনাকে পুরো হতে নিখুঁত হতে হবে না\n",
      "Original Data:===\n",
      " আজ আমাদের উঠোনে একটি ঠান্ডা, ক্লান্ত, ক্ষুধার্ত, আতঙ্কিত কালো বিড়াল পেয়েছে। সে সেখানে কয়েক দিন ছিল, কেউ তাকে মিস করছে? \n",
      "Cleaned Data:===\n",
      " আজ আমাদের উঠোনে একটি ঠান্ডা ক্লান্ত ক্ষুধার্ত আতঙ্কিত কালো বিড়াল পেয়েছে সে সেখানে কয়েক দিন ছিল কেউ তাকে মিস করছে\n",
      "Original Data:===\n",
      " কৃপণ অবস্থার জন্য টায়ার দুর্দান্ত সেট। \n",
      "Cleaned Data:===\n",
      " কৃপণ অবস্থার জন্য টায়ার দুর্দান্ত সেট\n",
      "Original Data:===\n",
      " চমত্কার তারা ক্যাবলেরো মে মাসে শক্তভাবে আবদ্ধ, ঠাট্টা, হতাশ এবং ধোঁয়াটে ফিরে আসবে \n",
      "Cleaned Data:===\n",
      " চমত্কার তারা ক্যাবলেরো মে মাসে শক্তভাবে আবদ্ধ ঠাট্টা হতাশ এবং ধোঁয়াটে ফিরে আসবে\n",
      "Original Data:===\n",
      " সম্প্রতি আমি একটি দুষ্ট টি-শার্ট পেয়েছি! ? তুমি এটা পছন্দ করো? ? \n",
      "Cleaned Data:===\n",
      " সম্প্রতি আমি একটি দুষ্ট টিশার্ট পেয়েছি তুমি এটা পছন্দ করো\n"
     ]
    }
   ],
   "source": [
    "#Removing punctuations\n",
    "train_data['cleaned'] = train_data['Captions'].apply(text_cleaning)\n",
    "valid_data['cleaned'] = valid_data['Captions'].apply(text_cleaning)\n",
    "test_data['cleaned'] = test_data['Captions'].apply(text_cleaning)\n",
    "\n",
    "\n",
    "## Data samples after cleaning\n",
    "print(\"Data samples after cleaning:\\n\")\n",
    "for i in range(10):\n",
    "  if i in train_data.index:\n",
    "    print(\"Original Data:===\\n\",train_data.Captions[i],\"\\nCleaned Data:===\\n\",train_data.cleaned[i],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gyyw4VqlFXS_"
   },
   "source": [
    "##Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "1rgUkMnlFbBU"
   },
   "outputs": [],
   "source": [
    "'''Evaluation Parameters'''\n",
    "\n",
    "def print_metrices(true,pred):\n",
    "    print(confusion_matrix(true,pred))\n",
    "    print(classification_report(true,pred))\n",
    "    print(\"Accuracy : \",accuracy_score(true,pred))\n",
    "    print(\"Precison : \",precision_score(true,pred, average = 'weighted'))\n",
    "    print(\"Recall : \",recall_score(true,pred,  average = 'weighted'))\n",
    "    print(\"F1 : \",f1_score(true,pred,  average = 'weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i27YJBb9V_jk"
   },
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "PwSSTl3kWB5T"
   },
   "outputs": [],
   "source": [
    "## Confusion matrix function\n",
    "def con_mat(true,pred,class_names,model_name):\n",
    "  cm = confusion_matrix(true,pred)\n",
    "  #sns.set()\n",
    "  plt.figure(figsize =(4, 3))\n",
    "  ax = plt.subplot()\n",
    " # 'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds','YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            #'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn'\n",
    "  sns.heatmap(cm, annot=True,fmt=\"d\",cmap='YlOrRd', ax = ax,annot_kws={\"size\": 10},) #annot=True to annotate cells\n",
    "  # labels, title and ticks\n",
    "  ax.set_xlabel('Predicted labels',fontsize=10)\n",
    "  ax.set_ylabel('True labels',fontsize=10) \n",
    "  #ax.set_title(f'Confusion Matrix of {model_name}',fontsize=10) \n",
    "  ax.xaxis.set_ticklabels(class_names, rotation=45); ax.yaxis.set_ticklabels(class_names, rotation=45);\n",
    "  ax.xaxis.tick_top()\n",
    " # plt.savefig(folder_path + f\"{model_name}.png\",bbox_inches='tight',dpi =500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cnsg6drRoiLt",
    "outputId": "97b60703-2791-4df7-8879-492d2d2090f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6359714617554338,\n",
       " 1: 1.147948487571129,\n",
       " 2: 1.5734811165845648,\n",
       " 3: 1.3689285714285715,\n",
       " 4: 0.881757533931447,\n",
       " 5: 1.3355400696864113,\n",
       " 6: 0.7647645650438947}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_data['enc_label']),\n",
    "                                        y = train_data['enc_label']                                                   \n",
    "                                    )\n",
    "class_weights = dict(zip(np.unique(train_data['enc_label']), class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGHsnStGRZP"
   },
   "source": [
    "#Textual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7UmwTi-GU7r"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEm2Xsd6Fddt",
    "outputId": "ff552eb6-0bf9-47aa-92b2-ae441fc0c98d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "\t\t\t====== Encoded Sequences ======\u001b[0m \n",
      "\n",
      "আজ সকালে একটি অন্ধকার লন্ডন কলনিতে আর্সেনালের প্রথম দলের ট্রেন আশা করি আমিরাতরা আগামীকাল হতাশ হবে না \n",
      " [31, 279, 6, 380, 1117, 4152, 4153, 152, 805, 806, 206, 18, 4154, 531, 72, 38, 5]\n",
      "\u001b[1m\n",
      "\t\t\t====== Paded Sequences ======\n",
      "\u001b[0m \n",
      " [  31  279    6  380 1117 4152 4153  152  805  806  206   18 4154  531\n",
      "   72   38    5    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "Number of Train Sequences : (3833, 180)\n",
      "Number of Train Sequences : (414, 180)\n",
      "Number of Test Sequences : (414, 180)\n",
      "Vocabulary Size:  11455\n"
     ]
    }
   ],
   "source": [
    "''' Tokenizer '''\n",
    "\n",
    "def text_tokenizer(train_data,valid_data,test_data,vocabulary,max_len,sample_text_num):\n",
    "  \n",
    "  tokenizer = Tokenizer(num_words = vocabulary ,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n-', \n",
    "                        split=' ', char_level=False, oov_token='<oov>', document_count=0)\n",
    "  tokenizer.fit_on_texts(train_data['cleaned'])  \n",
    "  word_index = tokenizer.word_index\n",
    "  vocab_size = len(word_index)+1\n",
    "  \n",
    "  # Training Sequences\n",
    "  train_sequences = tokenizer.texts_to_sequences(train_data['cleaned'])\n",
    "  train_pad_sequences =  keras.preprocessing.sequence.pad_sequences(train_sequences, value=0.0, padding='post', maxlen= max_len)\n",
    "\n",
    "\n",
    "  valid_sequences = tokenizer.texts_to_sequences(valid_data['cleaned'])\n",
    "  valid_pad_sequences =  keras.preprocessing.sequence.pad_sequences(valid_sequences, value=0.0, padding='post', maxlen= max_len)\n",
    "  \n",
    "  # Test Sequences\n",
    "  test_sequences = tokenizer.texts_to_sequences(test_data['cleaned'])\n",
    "  test_pad_sequences =  keras.preprocessing.sequence.pad_sequences(test_sequences, value=0.0, padding='post', maxlen= max_len)\n",
    " \n",
    "\n",
    "  print(color.BOLD+\"\\n\\t\\t\\t====== Encoded Sequences ======\"+color.END,\"\\n\")  \n",
    "  print(train_data.cleaned[sample_text_num],\"\\n\",train_sequences[sample_text_num])\n",
    "  print(color.BOLD+\"\\n\\t\\t\\t====== Paded Sequences ======\\n\"+color.END,\"\\n\",train_pad_sequences[sample_text_num]) \n",
    "\n",
    "  return train_pad_sequences, valid_pad_sequences, test_pad_sequences, vocab_size, word_index\n",
    "\n",
    "\n",
    "vocabulary = 50000 \n",
    "max_len = 180 \n",
    "sample_text_num = 10\n",
    "\n",
    "## Call Tokenizer\n",
    "train_pad_sequences,valid_pad_sequences, test_pad_sequences, vocab_size, word_index =  text_tokenizer(train_data,valid_data,test_data,\n",
    "                                                                      vocabulary,max_len,sample_text_num) \n",
    "\n",
    "print(\"Number of Train Sequences :\" ,train_pad_sequences.shape)\n",
    "print(\"Number of Train Sequences :\" ,valid_pad_sequences.shape)\n",
    "print(\"Number of Test Sequences :\" ,test_pad_sequences.shape)\n",
    "print(\"Vocabulary Size: \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "_TVt_7gWPqS-"
   },
   "outputs": [],
   "source": [
    "## Convert labels into array\n",
    "y_train = np.array(train_data['enc_label']).reshape(-1,1)\n",
    "y_valid = np.array(valid_data['enc_label']).reshape(-1,1)\n",
    "y_test = np.array(test_data['enc_label']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDf4t5nANV1N"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "3UdewIycNmIf"
   },
   "outputs": [],
   "source": [
    "''' Callbacks'''\n",
    "keras.backend.clear_session()\n",
    "def callbacks_check(model_name):\n",
    "  num_classes = 7\n",
    "  accuracy_threshold = 0.99\n",
    "\n",
    "  class myCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>accuracy_threshold):\n",
    "          print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold*100))\n",
    "          self.model.stop_training = True\n",
    "\n",
    "  acc_callback = myCallback()\n",
    "  # Saved the Best Model\n",
    "  filepath = models_path+'Models_3/'+f\"{model_name}.h5\"\n",
    "  checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True, \n",
    "                                             save_weights_only=False, mode='max')\n",
    "  # callback list\n",
    "  callback_list = [acc_callback, checkpoint] \n",
    "\n",
    "  return callback_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdjfy7SsTIan"
   },
   "source": [
    "## Textual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "9uKQhQWU9EzZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_keras_embeddings_models(max_len):\n",
    "\n",
    "  ###### BiLSTM Model #######\n",
    "  bi_text_inputs = Input(shape=(max_len,))\n",
    "  bi_embedding_layer = Embedding(vocab_size,100)(bi_text_inputs)\n",
    "  LSTM_Layer_1 = Bidirectional(LSTM(128,dropout=0.01))(bi_embedding_layer)\n",
    "  bi_dense_layer_1 = Dense(7, activation='softmax')(LSTM_Layer_1)\n",
    "  bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)\n",
    "\n",
    "\n",
    "  models = [bilstm_model]\n",
    " \n",
    "\n",
    "  return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "yzaDrX3dlrjH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train=to_categorical(train_data['enc_label'])\n",
    "y_valid=to_categorical(valid_data['enc_label'])\n",
    "y_test=to_categorical(test_data['enc_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VxoNP4ytlwJg",
    "outputId": "47ce668b-bfbe-46eb-8250-888cacc95f9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSLsF0v4cZ7d"
   },
   "source": [
    "#Visual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "gsRonbngccEl"
   },
   "outputs": [],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def visual_models():\n",
    "\n",
    "  #### Resnet\n",
    "  resnet = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "  #resnet.trainable = False\n",
    "  for layer in resnet.layers[0:-10]:\n",
    "      layer.trainable = False\n",
    "  # add a global spatial average pooling layer\n",
    "  y = resnet.output\n",
    "  pool1 = GlobalAveragePooling2D()(y)\n",
    "  # let's add a fully-connected layer\n",
    "  #flat1 = Flatten()(pool1)\n",
    "  # and a logistic layer -- let's say we have 200 classes\n",
    "  #hidden = Dense(512, activation='relu')(flat1)\n",
    "  #dropout1 = Dropout(0.1)(hidden)\n",
    "  output1 = Dense(7, activation='softmax')(pool1)\n",
    "  # this is the model we will train\n",
    "  resnet_img_model = Model(inputs=resnet.input, outputs=output1)\n",
    "\n",
    "\n",
    "\n",
    "# base_model = \n",
    "\n",
    "# # Add a global average pooling layer to reduce spatial dimensions\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# # Add a fully connected layer with softmax activation for classification\n",
    "# predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# # Create the model by specifying the input and output tensors\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "  models = [resnet_img_model]\n",
    "\n",
    "  return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdcsTCiFL1Rm",
    "outputId": "a55944c1-d43a-425b-f0e9-3faf4d66f1bf"
   },
   "outputs": [],
   "source": [
    "visual_model_name = ['resnet']\n",
    "visual_models_lsit = visual_models()\n",
    "\n",
    "# textual_model_name = ['LSTM','CNN','CNN_LSTM']\n",
    "textual_model_name = ['BiLSTM']\n",
    "text_models_list = create_keras_embeddings_models(180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5t4BKMwL1Ro",
    "outputId": "3c695d7d-eb11-4a13-dfb6-d9a87e738bc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: resnet+BiLSTM\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 01:36:01.144134: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - ETA: 0s - loss: 1.9447 - accuracy: 0.1492\n",
      "Epoch 1: val_accuracy improved from -inf to 0.16425, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 105s 800ms/step - loss: 1.9447 - accuracy: 0.1492 - val_loss: 1.9272 - val_accuracy: 0.1643\n",
      "Epoch 2/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.8915 - accuracy: 0.1936\n",
      "Epoch 2: val_accuracy improved from 0.16425 to 0.19082, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 111s 929ms/step - loss: 1.8915 - accuracy: 0.1936 - val_loss: 1.9574 - val_accuracy: 0.1908\n",
      "Epoch 3/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.8452 - accuracy: 0.2014\n",
      "Epoch 3: val_accuracy improved from 0.19082 to 0.30918, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 156s 1s/step - loss: 1.8452 - accuracy: 0.2014 - val_loss: 1.6489 - val_accuracy: 0.3092\n",
      "Epoch 4/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.6983 - accuracy: 0.2405\n",
      "Epoch 4: val_accuracy did not improve from 0.30918\n",
      "120/120 [==============================] - 146s 1s/step - loss: 1.6983 - accuracy: 0.2405 - val_loss: 1.5905 - val_accuracy: 0.2415\n",
      "Epoch 5/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.5476 - accuracy: 0.2570\n",
      "Epoch 5: val_accuracy improved from 0.30918 to 0.35749, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 144s 1s/step - loss: 1.5476 - accuracy: 0.2570 - val_loss: 1.5780 - val_accuracy: 0.3575\n",
      "Epoch 6/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.4629 - accuracy: 0.2969\n",
      "Epoch 6: val_accuracy did not improve from 0.35749\n",
      "120/120 [==============================] - 143s 1s/step - loss: 1.4629 - accuracy: 0.2969 - val_loss: 1.5482 - val_accuracy: 0.2657\n",
      "Epoch 7/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.4222 - accuracy: 0.2930\n",
      "Epoch 7: val_accuracy did not improve from 0.35749\n",
      "120/120 [==============================] - 145s 1s/step - loss: 1.4222 - accuracy: 0.2930 - val_loss: 1.5578 - val_accuracy: 0.2295\n",
      "Epoch 8/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.3778 - accuracy: 0.3201\n",
      "Epoch 8: val_accuracy did not improve from 0.35749\n",
      "120/120 [==============================] - 146s 1s/step - loss: 1.3778 - accuracy: 0.3201 - val_loss: 1.5365 - val_accuracy: 0.2560\n",
      "Epoch 9/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.3483 - accuracy: 0.3183\n",
      "Epoch 9: val_accuracy did not improve from 0.35749\n",
      "120/120 [==============================] - 144s 1s/step - loss: 1.3483 - accuracy: 0.3183 - val_loss: 1.5207 - val_accuracy: 0.2681\n",
      "Epoch 10/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.2903 - accuracy: 0.3707\n",
      "Epoch 10: val_accuracy improved from 0.35749 to 0.44928, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 149s 1s/step - loss: 1.2903 - accuracy: 0.3707 - val_loss: 1.4533 - val_accuracy: 0.4493\n",
      "Epoch 11/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.2267 - accuracy: 0.4096\n",
      "Epoch 11: val_accuracy improved from 0.44928 to 0.46377, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 147s 1s/step - loss: 1.2267 - accuracy: 0.4096 - val_loss: 1.3952 - val_accuracy: 0.4638\n",
      "Epoch 12/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.1459 - accuracy: 0.4307\n",
      "Epoch 12: val_accuracy did not improve from 0.46377\n",
      "120/120 [==============================] - 145s 1s/step - loss: 1.1459 - accuracy: 0.4307 - val_loss: 1.3503 - val_accuracy: 0.4565\n",
      "Epoch 13/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.0694 - accuracy: 0.4709\n",
      "Epoch 13: val_accuracy did not improve from 0.46377\n",
      "120/120 [==============================] - 153s 1s/step - loss: 1.0694 - accuracy: 0.4709 - val_loss: 1.3862 - val_accuracy: 0.3961\n",
      "Epoch 14/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.0352 - accuracy: 0.4738\n",
      "Epoch 14: val_accuracy did not improve from 0.46377\n",
      "120/120 [==============================] - 155s 1s/step - loss: 1.0352 - accuracy: 0.4738 - val_loss: 1.3292 - val_accuracy: 0.4493\n",
      "Epoch 15/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.9812 - accuracy: 0.5043\n",
      "Epoch 15: val_accuracy improved from 0.46377 to 0.47343, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 150s 1s/step - loss: 0.9812 - accuracy: 0.5043 - val_loss: 1.3445 - val_accuracy: 0.4734\n",
      "Epoch 16/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.9324 - accuracy: 0.5633\n",
      "Epoch 16: val_accuracy improved from 0.47343 to 0.57246, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 152s 1s/step - loss: 0.9324 - accuracy: 0.5633 - val_loss: 1.2620 - val_accuracy: 0.5725\n",
      "Epoch 17/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.8639 - accuracy: 0.6162\n",
      "Epoch 17: val_accuracy did not improve from 0.57246\n",
      "120/120 [==============================] - 160s 1s/step - loss: 0.8639 - accuracy: 0.6162 - val_loss: 1.3059 - val_accuracy: 0.5459\n",
      "Epoch 18/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.8056 - accuracy: 0.6614\n",
      "Epoch 18: val_accuracy did not improve from 0.57246\n",
      "120/120 [==============================] - 154s 1s/step - loss: 0.8056 - accuracy: 0.6614 - val_loss: 1.2976 - val_accuracy: 0.5483\n",
      "Epoch 19/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.7409 - accuracy: 0.6606\n",
      "Epoch 19: val_accuracy did not improve from 0.57246\n",
      "120/120 [==============================] - 152s 1s/step - loss: 0.7409 - accuracy: 0.6606 - val_loss: 1.3592 - val_accuracy: 0.5628\n",
      "Epoch 20/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.6859\n",
      "Epoch 20: val_accuracy did not improve from 0.57246\n",
      "120/120 [==============================] - 151s 1s/step - loss: 0.6898 - accuracy: 0.6859 - val_loss: 1.3343 - val_accuracy: 0.5531\n",
      "Epoch 21/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.7589\n",
      "Epoch 21: val_accuracy did not improve from 0.57246\n",
      "120/120 [==============================] - 152s 1s/step - loss: 0.5997 - accuracy: 0.7589 - val_loss: 1.4779 - val_accuracy: 0.5169\n",
      "Epoch 22/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5498 - accuracy: 0.7527\n",
      "Epoch 22: val_accuracy improved from 0.57246 to 0.62802, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 162s 1s/step - loss: 0.5498 - accuracy: 0.7527 - val_loss: 1.4977 - val_accuracy: 0.6280\n",
      "Epoch 23/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5124 - accuracy: 0.7725\n",
      "Epoch 23: val_accuracy did not improve from 0.62802\n",
      "120/120 [==============================] - 158s 1s/step - loss: 0.5124 - accuracy: 0.7725 - val_loss: 1.6071 - val_accuracy: 0.5604\n",
      "Epoch 24/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.7842\n",
      "Epoch 24: val_accuracy did not improve from 0.62802\n",
      "120/120 [==============================] - 158s 1s/step - loss: 0.4725 - accuracy: 0.7842 - val_loss: 1.5390 - val_accuracy: 0.5918\n",
      "Epoch 25/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.7741\n",
      "Epoch 25: val_accuracy did not improve from 0.62802\n",
      "120/120 [==============================] - 160s 1s/step - loss: 0.4583 - accuracy: 0.7741 - val_loss: 1.6278 - val_accuracy: 0.5990\n",
      "Epoch 26/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.7981\n",
      "Epoch 26: val_accuracy did not improve from 0.62802\n",
      "120/120 [==============================] - 158s 1s/step - loss: 0.4270 - accuracy: 0.7981 - val_loss: 1.6053 - val_accuracy: 0.5894\n",
      "Epoch 27/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.7952\n",
      "Epoch 27: val_accuracy improved from 0.62802 to 0.64493, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 157s 1s/step - loss: 0.3981 - accuracy: 0.7952 - val_loss: 1.6445 - val_accuracy: 0.6449\n",
      "Epoch 28/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8109\n",
      "Epoch 28: val_accuracy did not improve from 0.64493\n",
      "120/120 [==============================] - 162s 1s/step - loss: 0.3914 - accuracy: 0.8109 - val_loss: 1.7549 - val_accuracy: 0.5797\n",
      "Epoch 29/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8012\n",
      "Epoch 29: val_accuracy improved from 0.64493 to 0.65459, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 154s 1s/step - loss: 0.3689 - accuracy: 0.8012 - val_loss: 1.6068 - val_accuracy: 0.6546\n",
      "Epoch 30/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.8135\n",
      "Epoch 30: val_accuracy did not improve from 0.65459\n",
      "120/120 [==============================] - 155s 1s/step - loss: 0.3625 - accuracy: 0.8135 - val_loss: 1.6657 - val_accuracy: 0.5942\n",
      "Epoch 31/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.8111\n",
      "Epoch 31: val_accuracy did not improve from 0.65459\n",
      "120/120 [==============================] - 150s 1s/step - loss: 0.3531 - accuracy: 0.8111 - val_loss: 1.7335 - val_accuracy: 0.6159\n",
      "Epoch 32/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.8119\n",
      "Epoch 32: val_accuracy did not improve from 0.65459\n",
      "120/120 [==============================] - 152s 1s/step - loss: 0.3514 - accuracy: 0.8119 - val_loss: 1.6763 - val_accuracy: 0.5966\n",
      "Epoch 33/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.8195\n",
      "Epoch 33: val_accuracy did not improve from 0.65459\n",
      "120/120 [==============================] - 156s 1s/step - loss: 0.3459 - accuracy: 0.8195 - val_loss: 1.7199 - val_accuracy: 0.6159\n",
      "Epoch 34/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.8182\n",
      "Epoch 34: val_accuracy improved from 0.65459 to 0.66908, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 156s 1s/step - loss: 0.3540 - accuracy: 0.8182 - val_loss: 1.6505 - val_accuracy: 0.6691\n",
      "Epoch 35/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.8283\n",
      "Epoch 35: val_accuracy did not improve from 0.66908\n",
      "120/120 [==============================] - 160s 1s/step - loss: 0.3517 - accuracy: 0.8283 - val_loss: 1.7210 - val_accuracy: 0.6063\n",
      "Epoch 36/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3286 - accuracy: 0.8325\n",
      "Epoch 36: val_accuracy did not improve from 0.66908\n",
      "120/120 [==============================] - 159s 1s/step - loss: 0.3286 - accuracy: 0.8325 - val_loss: 1.7332 - val_accuracy: 0.6256\n",
      "Epoch 37/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.8354\n",
      "Epoch 37: val_accuracy did not improve from 0.66908\n",
      "120/120 [==============================] - 153s 1s/step - loss: 0.3168 - accuracy: 0.8354 - val_loss: 1.6916 - val_accuracy: 0.6522\n",
      "Epoch 38/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.8396\n",
      "Epoch 38: val_accuracy did not improve from 0.66908\n",
      "120/120 [==============================] - 141s 1s/step - loss: 0.3095 - accuracy: 0.8396 - val_loss: 1.6089 - val_accuracy: 0.6594\n",
      "Epoch 39/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.8529\n",
      "Epoch 39: val_accuracy did not improve from 0.66908\n",
      "120/120 [==============================] - 141s 1s/step - loss: 0.2998 - accuracy: 0.8529 - val_loss: 1.6636 - val_accuracy: 0.6667\n",
      "Epoch 40/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3252 - accuracy: 0.8330\n",
      "Epoch 40: val_accuracy improved from 0.66908 to 0.68116, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 143s 1s/step - loss: 0.3252 - accuracy: 0.8330 - val_loss: 1.7058 - val_accuracy: 0.6812\n",
      "Epoch 41/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.8442\n",
      "Epoch 41: val_accuracy did not improve from 0.68116\n",
      "120/120 [==============================] - 140s 1s/step - loss: 0.3146 - accuracy: 0.8442 - val_loss: 1.6935 - val_accuracy: 0.6618\n",
      "Epoch 42/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.8523\n",
      "Epoch 42: val_accuracy did not improve from 0.68116\n",
      "120/120 [==============================] - 141s 1s/step - loss: 0.2933 - accuracy: 0.8523 - val_loss: 1.6835 - val_accuracy: 0.6473\n",
      "Epoch 43/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.8591\n",
      "Epoch 43: val_accuracy did not improve from 0.68116\n",
      "120/120 [==============================] - 139s 1s/step - loss: 0.2801 - accuracy: 0.8591 - val_loss: 1.6758 - val_accuracy: 0.6498\n",
      "Epoch 44/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8549\n",
      "Epoch 44: val_accuracy did not improve from 0.68116\n",
      "120/120 [==============================] - 139s 1s/step - loss: 0.2856 - accuracy: 0.8549 - val_loss: 1.6432 - val_accuracy: 0.6763\n",
      "Epoch 45/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.8604\n",
      "Epoch 45: val_accuracy improved from 0.68116 to 0.68841, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 143s 1s/step - loss: 0.2872 - accuracy: 0.8604 - val_loss: 1.6396 - val_accuracy: 0.6884\n",
      "Epoch 46/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.8750\n",
      "Epoch 46: val_accuracy did not improve from 0.68841\n",
      "120/120 [==============================] - 163s 1s/step - loss: 0.2562 - accuracy: 0.8750 - val_loss: 1.7222 - val_accuracy: 0.6836\n",
      "Epoch 47/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8740\n",
      "Epoch 47: val_accuracy did not improve from 0.68841\n",
      "120/120 [==============================] - 160s 1s/step - loss: 0.2769 - accuracy: 0.8740 - val_loss: 1.5963 - val_accuracy: 0.6812\n",
      "Epoch 48/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.8774\n",
      "Epoch 48: val_accuracy did not improve from 0.68841\n",
      "120/120 [==============================] - 160s 1s/step - loss: 0.2751 - accuracy: 0.8774 - val_loss: 1.5344 - val_accuracy: 0.6787\n",
      "Epoch 49/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.9082\n",
      "Epoch 49: val_accuracy improved from 0.68841 to 0.70531, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 159s 1s/step - loss: 0.2262 - accuracy: 0.9082 - val_loss: 1.6287 - val_accuracy: 0.7053\n",
      "Epoch 50/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9369\n",
      "Epoch 50: val_accuracy improved from 0.70531 to 0.74396, saving model to /Users/zaima/Desktop/Dataset/Multimodal Sentiment/Models_3/resnetBiLSTM.h5\n",
      "120/120 [==============================] - 159s 1s/step - loss: 0.1958 - accuracy: 0.9369 - val_loss: 1.4919 - val_accuracy: 0.7440\n"
     ]
    }
   ],
   "source": [
    "for vm,visual_model in enumerate(visual_models_lsit):\n",
    "  for tm,textual_model in enumerate(text_models_list):\n",
    "    # Concatenating the output of 2 classifiers\n",
    "    con_layer = keras.layers.concatenate([visual_model.output, textual_model.output])\n",
    "    dropout = Dropout(0.2)(con_layer)\n",
    "    final_dense = Dense(64, activation=\"relu\")(dropout)\n",
    "    out = Dense(7,activation='softmax')(final_dense)\n",
    "     \n",
    "    #Defining model input and output\n",
    "    com_model = Model(inputs = [visual_model.input, textual_model.input], outputs=out)\n",
    "   \n",
    "      \n",
    "    com_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics = [\"accuracy\"])\n",
    "    print(f\"Model Name: {visual_model_name[vm]}+{textual_model_name[tm]}\\n\")\n",
    "    com_model.fit([train_image,train_pad_sequences],\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size =32,\n",
    "    validation_data=([valid_image,valid_pad_sequences],y_valid),\n",
    "    verbose =1,\n",
    "    class_weight = class_weights,\n",
    "    callbacks = callbacks_check(visual_model_name[vm]+textual_model_name[tm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-CzBY_R2H6g"
   },
   "source": [
    "## Multimodal Models Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "QGo23SUS90UI"
   },
   "outputs": [],
   "source": [
    "multimodal_models = ['resnetBiLSTM']\n",
    "multimodal_names =  ['ResNet+BiLSTM']\n",
    "\n",
    "def multimodal_models_accuracy(saved_model): \n",
    "  my_dict = {}\n",
    "  # Prediction \n",
    "  model = load_model(models_path+'Models_3/'+f\"{saved_model}.h5\")\n",
    "  y_pred = np.argmax(model.predict([test_image,test_pad_sequences]), axis=-1)\n",
    "\n",
    "  y_true = test_data['enc_label']\n",
    "\n",
    "  my_dict['Accuracy'] = accuracy_score(y_true, y_pred)*100\n",
    "  my_dict['Precision'] = precision_score(y_true, y_pred,average = 'weighted')*100\n",
    "  my_dict['Recall'] = recall_score(y_true, y_pred,average = 'weighted')*100 \n",
    "  my_dict['F1 Score'] = f1_score(y_true, y_pred,average = 'weighted')*100 \n",
    "  return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZk_EjCB2H6l",
    "outputId": "e2314b89-204d-4abe-e065-a50ce105da7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 17s 833ms/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = {f'{multimodal_names[i]}':multimodal_models_accuracy(model) for i,model in enumerate(multimodal_models)}\n",
    "# Save the performance parameter into json file\n",
    "with open(models_path+'Results_3/'+'anurag_models_performance.json', 'w') as f:\n",
    "    json.dump(accuracy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=======  Multimodal Models Performance on Test Data  =============\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ResNet+BiLSTM</th>\n",
       "      <td>75.53</td>\n",
       "      <td>76.01</td>\n",
       "      <td>75.53</td>\n",
       "      <td>75.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision  Recall  F1 Score\n",
       "ResNet+BiLSTM     75.53      76.01   75.53     75.86"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the json file\n",
    "metrics = json.load(open(models_path+'Results_3/'+'anurag_models_performance.json'))\n",
    "acc_list = []\n",
    "pr_list = []\n",
    "re_list = []\n",
    "f1_list = []\n",
    "for i in metrics.keys():\n",
    "  acc_list.append(round(metrics[i]['Accuracy'],2))\n",
    "  pr_list.append(round(metrics[i]['Precision'],2))\n",
    "  re_list.append(round(metrics[i]['Recall'],2))\n",
    "  f1_list.append(round(metrics[i]['F1 Score'],2))\n",
    "\n",
    "print (color.BOLD+f\"=======  Multimodal Models Performance on Test Data  =============\\n\"+color.END)\n",
    "# Create a dataframe\n",
    "performance_matrix = pd.DataFrame({'Accuracy':acc_list,'Precision':pr_list,\n",
    "                                   'Recall':re_list,'F1 Score':f1_list},\n",
    "                                  index =multimodal_names)\n",
    "performance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
